<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Data | Transformer Implementation with the High-Level Keras API</title>
  <meta name="description" content="This is an transformer implementation from scratch using the Keras API." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Data | Transformer Implementation with the High-Level Keras API" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an transformer implementation from scratch using the Keras API." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Data | Transformer Implementation with the High-Level Keras API" />
  
  <meta name="twitter:description" content="This is an transformer implementation from scratch using the Keras API." />
  

<meta name="author" content="James Hirschorn" />


<meta name="date" content="2021-06-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="implementation.html"/>
<link rel="next" href="architecture.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>1</b> Transformer Implementation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="implementation.html"><a href="implementation.html#requirements"><i class="fa fa-check"></i><b>1.1</b> Requirements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data.html"><a href="data.html#tokenizers"><i class="fa fa-check"></i><b>2.1</b> Tokenizers</a></li>
<li class="chapter" data-level="2.2" data-path="data.html"><a href="data.html#data-pipeline"><i class="fa fa-check"></i><b>2.2</b> Data Pipeline</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="architecture.html"><a href="architecture.html"><i class="fa fa-check"></i><b>3</b> Transformer Architecture</a>
<ul>
<li class="chapter" data-level="3.1" data-path="architecture.html"><a href="architecture.html#embeddings"><i class="fa fa-check"></i><b>3.1</b> Embeddings</a></li>
<li class="chapter" data-level="3.2" data-path="architecture.html"><a href="architecture.html#masking"><i class="fa fa-check"></i><b>3.2</b> Masking</a></li>
<li class="chapter" data-level="3.3" data-path="architecture.html"><a href="architecture.html#positional-encodings"><i class="fa fa-check"></i><b>3.3</b> Positional Encodings</a></li>
<li class="chapter" data-level="3.4" data-path="architecture.html"><a href="architecture.html#transformer-sublayers"><i class="fa fa-check"></i><b>3.4</b> Transformer Sublayers</a></li>
<li class="chapter" data-level="3.5" data-path="architecture.html"><a href="architecture.html#attention"><i class="fa fa-check"></i><b>3.5</b> Attention</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="architecture.html"><a href="architecture.html#scaled-dot-product-attention"><i class="fa fa-check"></i><b>3.5.1</b> Scaled Dot-Product Attention</a></li>
<li class="chapter" data-level="3.5.2" data-path="architecture.html"><a href="architecture.html#attention-layer"><i class="fa fa-check"></i><b>3.5.2</b> Attention Layer</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="architecture.html"><a href="architecture.html#feed-forward-networks"><i class="fa fa-check"></i><b>3.6</b> Feed-Forward Networks</a></li>
<li class="chapter" data-level="3.7" data-path="architecture.html"><a href="architecture.html#encoder"><i class="fa fa-check"></i><b>3.7</b> Encoder</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="architecture.html"><a href="architecture.html#encoder-layer"><i class="fa fa-check"></i><b>3.7.1</b> Encoder Layer</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="architecture.html"><a href="architecture.html#decoder"><i class="fa fa-check"></i><b>3.8</b> Decoder</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="architecture.html"><a href="architecture.html#decoder-layer"><i class="fa fa-check"></i><b>3.8.1</b> Decoder Layer</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="architecture.html"><a href="architecture.html#transformer-model"><i class="fa fa-check"></i><b>3.9</b> Transformer Model</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model.html"><a href="model.html"><i class="fa fa-check"></i><b>4</b> Model Usage</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model.html"><a href="model.html#training"><i class="fa fa-check"></i><b>4.1</b> Training</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model.html"><a href="model.html#loss"><i class="fa fa-check"></i><b>4.1.1</b> Loss</a></li>
<li class="chapter" data-level="4.1.2" data-path="model.html"><a href="model.html#optimization"><i class="fa fa-check"></i><b>4.1.2</b> Optimization</a></li>
<li class="chapter" data-level="4.1.3" data-path="model.html"><a href="model.html#learning"><i class="fa fa-check"></i><b>4.1.3</b> Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model.html"><a href="model.html#inference"><i class="fa fa-check"></i><b>4.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transformer Implementation with the High-Level Keras API</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Data<a href="data.html#data" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>All of the data is obtained from the <a href="https://www.tensorflow.org/datasets" title="TensorFlow Datasets: a collection of ready-to-use datasets"><code>tensorflow_datasets</code></a> library. We begin with the <a href="https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate" title="Data sets derived from TED talk transcripts for comparing similar language pairs where one is high resource and the other is low resource"><code>ted_hrlr_translate</code></a> resource and the Portuguese to English language pair.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="data.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-2"><a href="data.html#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="data.html#cb1-3" aria-hidden="true" tabindex="-1"></a>resource <span class="op">=</span> <span class="st">&#39;ted_hrlr_translate&#39;</span></span>
<span id="cb1-4"><a href="data.html#cb1-4" aria-hidden="true" tabindex="-1"></a>pair <span class="op">=</span> <span class="st">&#39;pt_to_en&#39;</span></span>
<span id="cb1-5"><a href="data.html#cb1-5" aria-hidden="true" tabindex="-1"></a>examples, metadata <span class="op">=</span> tfds.load(<span class="ss">f&#39;</span><span class="sc">{</span>resource<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>pair<span class="sc">}</span><span class="ss">&#39;</span>, with_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-6"><a href="data.html#cb1-6" aria-hidden="true" tabindex="-1"></a>                               as_supervised<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-7"><a href="data.html#cb1-7" aria-hidden="true" tabindex="-1"></a>                               </span>
<span id="cb1-8"><a href="data.html#cb1-8" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> metadata.supervised_keys</span>
<span id="cb1-9"><a href="data.html#cb1-9" aria-hidden="true" tabindex="-1"></a>train_examples, eval_examples <span class="op">=</span> examples[<span class="st">&#39;train&#39;</span>], examples[<span class="st">&#39;validation&#39;</span>]</span>
<span id="cb1-10"><a href="data.html#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="data.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Keys: </span><span class="sc">{</span>metadata<span class="sc">.</span>supervised_keys<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## Keys: (&#39;pt&#39;, &#39;en&#39;)</code></pre>
<p>The individual examples have the following format:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="data.html#cb3-1" aria-hidden="true" tabindex="-1"></a>example1 <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_examples))</span>
<span id="cb3-2"><a href="data.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(example1)</span></code></pre></div>
<pre><code>## (&lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;e quando melhoramos a procura ,
## tiramos a \xc3\xbanica vantagem da impress\xc3\xa3o , que \xc3\xa9 a
## serendipidade .&#39;&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b&#39;and when you
## improve searchability , you actually take away the one advantage of print ,
## which is serendipity .&#39;&gt;)</code></pre>
<div id="tokenizers" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Tokenizers<a href="data.html#tokenizers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As usual for language modeling, sentences in some language must be converted to sequences of integers in order to serve as input for a neural network, in a process called <em>tokenization</em>.
The input sentences are tokenized using the class <code>SubwordTokenizer</code> in the script <a href="https://github.com/quantitative-technologies/transformer-high-level-keras-api/blob/master/inst/python/tokenizer/subword_tokenizer.py"><code>tokenizer/subword_tokenizer.py</code></a>. It is closely based on the <code>CustomTokenizer</code> class from the <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer" title="Subword tokenizers">Subword tokenizer tutorial</a> which is in turn based on the <code>BertTokenizer</code> from <code>tensorflow_text</code>.</p>
<p>From the tutorial: “The main advantage of a subword tokenizer is that it interpolates between word-based and character-based tokenization. Common words get a slot in the vocabulary, but the tokenizer can fall back to word pieces and individual characters for unknown words.” <code>SubwordTokenizer</code> takes a sentence and first splits it into words using BERT’s token splitting algorithm and then applies a subword tokenizer using the <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer#applying_wordpiece">WordPiece algorithm</a>.</p>
<p>The script <a href="https://github.com/quantitative-technologies/transformer-high-level-keras-api/blob/master/inst/python/prepare_tokenizers.py"><code>prepare_tokenizers.py</code></a> provides the <code>prepare_tokenizers</code> function which builds a pair of <code>SubwordTokenizer</code>s from the input examples and saves them to disk for later reuse, as they take some time to build. The parameters below indicate that all text is converted to lowercase and that the maximum vocabulary size of both the inputs and targets is <span class="math inline">\(2^{13} = 8192\)</span>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="data.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> prepare_tokenizers <span class="im">import</span> prepare_tokenizers</span>
<span id="cb5-2"><a href="data.html#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="data.html#cb5-3" aria-hidden="true" tabindex="-1"></a>TRAIN_DIR <span class="op">=</span> <span class="st">&#39;train&#39;</span></span>
<span id="cb5-4"><a href="data.html#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="data.html#cb5-5" aria-hidden="true" tabindex="-1"></a>tokenizers, _ <span class="op">=</span> prepare_tokenizers(train_examples,</span>
<span id="cb5-6"><a href="data.html#cb5-6" aria-hidden="true" tabindex="-1"></a>                                   lower_case<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-7"><a href="data.html#cb5-7" aria-hidden="true" tabindex="-1"></a>                                   input_vocab_size<span class="op">=</span><span class="dv">2</span> <span class="op">**</span> <span class="dv">13</span>,</span>
<span id="cb5-8"><a href="data.html#cb5-8" aria-hidden="true" tabindex="-1"></a>                                   target_vocab_size<span class="op">=</span><span class="dv">2</span> <span class="op">**</span> <span class="dv">13</span>,</span>
<span id="cb5-9"><a href="data.html#cb5-9" aria-hidden="true" tabindex="-1"></a>                                   name<span class="op">=</span>metadata.name <span class="op">+</span> <span class="st">&#39;-&#39;</span> <span class="op">+</span> keys[<span class="dv">0</span>] <span class="op">+</span> <span class="st">&#39;_to_&#39;</span> <span class="op">+</span> keys[<span class="dv">1</span>],</span>
<span id="cb5-10"><a href="data.html#cb5-10" aria-hidden="true" tabindex="-1"></a>                                   tokenizer_dir<span class="op">=</span>TRAIN_DIR,</span>
<span id="cb5-11"><a href="data.html#cb5-11" aria-hidden="true" tabindex="-1"></a>                                   reuse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-12"><a href="data.html#cb5-12" aria-hidden="true" tabindex="-1"></a>                                </span>
<span id="cb5-13"><a href="data.html#cb5-13" aria-hidden="true" tabindex="-1"></a>input_vocab_size <span class="op">=</span> tokenizers.inputs.get_vocab_size()</span>
<span id="cb5-14"><a href="data.html#cb5-14" aria-hidden="true" tabindex="-1"></a>target_vocab_size <span class="op">=</span> tokenizers.targets.get_vocab_size()</span>
<span id="cb5-15"><a href="data.html#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of input tokens: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(input_vocab_size))</span></code></pre></div>
<pre><code>## Number of input tokens: 8318</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="data.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of target tokens: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(target_vocab_size))</span></code></pre></div>
<pre><code>## Number of target tokens: 7010</code></pre>
<p>The tokenizer is demonstrated on the the English sentence from example 1 above.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="data.html#cb9-1" aria-hidden="true" tabindex="-1"></a>example1_en_string <span class="op">=</span> example1[<span class="dv">1</span>].numpy().decode(<span class="st">&#39;utf-8&#39;</span>)</span>
<span id="cb9-2"><a href="data.html#cb9-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> tokenizers.targets</span>
<span id="cb9-3"><a href="data.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Sentence: </span><span class="sc">{</span>example1_en_string<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## Sentence: and when you improve searchability , you actually take away the one
## advantage of print , which is serendipity .</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="data.html#cb11-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.tokenize([example1_en_string])</span>
<span id="cb11-2"><a href="data.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Tokenized sentence: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## Tokenized sentence: &lt;tf.RaggedTensor [[2, 72, 117, 79, 1259, 1491, 2362, 13,
## 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423,
## 540, 15, 3]]&gt;</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="data.html#cb13-1" aria-hidden="true" tabindex="-1"></a>text_tokens <span class="op">=</span> tokenizer.lookup(tokens)</span>
<span id="cb13-2"><a href="data.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Text tokens: </span><span class="sc">{</span>text_tokens<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## Text tokens: &lt;tf.RaggedTensor [[b&#39;[START]&#39;, b&#39;and&#39;, b&#39;when&#39;, b&#39;you&#39;,
## b&#39;improve&#39;, b&#39;search&#39;, b&#39;##ability&#39;, b&#39;,&#39;, b&#39;you&#39;, b&#39;actually&#39;, b&#39;take&#39;,
## b&#39;away&#39;, b&#39;the&#39;, b&#39;one&#39;, b&#39;advantage&#39;, b&#39;of&#39;, b&#39;print&#39;, b&#39;,&#39;, b&#39;which&#39;, b&#39;is&#39;,
## b&#39;s&#39;, b&#39;##ere&#39;, b&#39;##nd&#39;, b&#39;##ip&#39;, b&#39;##ity&#39;, b&#39;.&#39;, b&#39;[END]&#39;]]&gt;</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="data.html#cb15-1" aria-hidden="true" tabindex="-1"></a>round_trip <span class="op">=</span> tokenizer.detokenize(tokens)</span>
<span id="cb15-2"><a href="data.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Convert tokens back to original sentence: &quot;</span> \</span>
<span id="cb15-3"><a href="data.html#cb15-3" aria-hidden="true" tabindex="-1"></a>      <span class="ss">f&quot;</span><span class="sc">{</span>round_trip<span class="sc">.</span>numpy()[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">.</span>decode(<span class="st">&#39;utf-8&#39;</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Convert tokens back to original sentence: and when you improve searchability ,
## you actually take away the one advantage of print , which is serendipity .</code></pre>
<p>The <code>tokenize</code> method converts a sentence (or any block of text) into a sequence of tokens (i.e. integers).
The <code>SubwordTokenizer</code> methods are intended for lists of sentences, corresponding to the batched inputs fed to the neural network, while in this example we use a batch of size one.
The <code>lookup</code> method shows which subword each input token represents.
Note that the tokenizer has added special start and end tokens accordingly to the tokenized sequence, which allows the model to understand about the start and end of each input.
<code>detokenize</code> maps the tokens back to the original sentence.</p>
</div>
<div id="data-pipeline" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Data Pipeline<a href="data.html#data-pipeline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" title="tf.data.Dataset"><code>tf.data.Dataset</code></a> API is used for the input pipeline, suitable for consumption by <code>TensorFlow</code>/<code>Keras</code> models. Since our data comes from <code>tensorflow_datasets</code> it is already a <code>tf.data</code> object to which we can apply the necessary transformations and then iterate as batches.</p>
<p>Our input pipeline tokenizes the sentences from both languages into sequences of integers, discards any examples where either the source or target has more than <code>MAX_LEN</code> tokens and collects them into batches of size <code>BATCH_SIZE</code>. The reason for limiting the length of the input sequences is that both the transformer run time and memory usage are quadratic in the input length, which is evident from the attention mechanism shown in equation <a href="architecture.html#eq:attention">(3.1)</a> below.</p>
<p>The result is a <code>tf.data</code> dataset which return a tuple of <code>(inputs, targets)</code> for each batch. As is typical for Encoder–Decoder auto-regressive sequence-to-sequence architectures, the input is of the form <code>(encoder_inpout, decoder_input)</code> where <code>encoder_input</code> is the tokenized source sentence and <code>decoder_input</code> is tokenized target sentence with the last token dropped;
while <code>targets</code> is the tokenized target sentence lagged by one for autoregression.</p>
<p>The input pipeline encapsulated in our <code>Dataset</code> class follows the <a href="https://www.tensorflow.org/guide/data_performance" title="Better performance with the tf.data API">TensorFlow Data Pipeline Performance Guide</a>:</p>
<p><strong><a href="https://github.com/quantitative-technologies/transformer-high-level-keras-api/blob/master/inst/python/transformer/dataset.py"><code>transformer/dataset.py</code></a></strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="data.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb17-2"><a href="data.html#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="data.html#cb17-3" aria-hidden="true" tabindex="-1"></a>BUFFER_SIZE <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb17-4"><a href="data.html#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="data.html#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="data.html#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dataset:</span>
<span id="cb17-7"><a href="data.html#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb17-8"><a href="data.html#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Provides a data pipeline suitable for use with transformers</span></span>
<span id="cb17-9"><a href="data.html#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb17-10"><a href="data.html#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokenizers, batch_size, input_seqlen, target_seqlen):</span>
<span id="cb17-11"><a href="data.html#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizers <span class="op">=</span> tokenizers</span>
<span id="cb17-12"><a href="data.html#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb17-13"><a href="data.html#cb17-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_seqlen <span class="op">=</span> input_seqlen</span>
<span id="cb17-14"><a href="data.html#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_seqlen <span class="op">=</span> target_seqlen</span>
<span id="cb17-15"><a href="data.html#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="data.html#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> data_pipeline(<span class="va">self</span>, examples, num_parallel_calls<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-17"><a href="data.html#cb17-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (</span>
<span id="cb17-18"><a href="data.html#cb17-18" aria-hidden="true" tabindex="-1"></a>            examples</span>
<span id="cb17-19"><a href="data.html#cb17-19" aria-hidden="true" tabindex="-1"></a>                .cache()</span>
<span id="cb17-20"><a href="data.html#cb17-20" aria-hidden="true" tabindex="-1"></a>                .<span class="bu">map</span>(tokenize_pairs(<span class="va">self</span>.tokenizers),</span>
<span id="cb17-21"><a href="data.html#cb17-21" aria-hidden="true" tabindex="-1"></a>                     num_parallel_calls<span class="op">=</span>num_parallel_calls)</span>
<span id="cb17-22"><a href="data.html#cb17-22" aria-hidden="true" tabindex="-1"></a>                .<span class="bu">filter</span>(filter_max_length(max_x_length<span class="op">=</span><span class="va">self</span>.input_seqlen,</span>
<span id="cb17-23"><a href="data.html#cb17-23" aria-hidden="true" tabindex="-1"></a>                                          max_y_length<span class="op">=</span><span class="va">self</span>.target_seqlen))</span>
<span id="cb17-24"><a href="data.html#cb17-24" aria-hidden="true" tabindex="-1"></a>                .shuffle(BUFFER_SIZE)</span>
<span id="cb17-25"><a href="data.html#cb17-25" aria-hidden="true" tabindex="-1"></a>                .padded_batch(<span class="va">self</span>.batch_size)</span>
<span id="cb17-26"><a href="data.html#cb17-26" aria-hidden="true" tabindex="-1"></a>                .prefetch(tf.data.AUTOTUNE)</span>
<span id="cb17-27"><a href="data.html#cb17-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-28"><a href="data.html#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="data.html#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="data.html#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> filter_max_length(max_x_length, max_y_length):</span>
<span id="cb17-31"><a href="data.html#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">filter</span>(x, y):</span>
<span id="cb17-32"><a href="data.html#cb17-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.logical_and(tf.size(x[<span class="st">&#39;encoder_input&#39;</span>]) <span class="op">&lt;=</span> max_x_length,</span>
<span id="cb17-33"><a href="data.html#cb17-33" aria-hidden="true" tabindex="-1"></a>                              tf.size(y) <span class="op">&lt;</span> max_y_length)</span>
<span id="cb17-34"><a href="data.html#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="data.html#cb17-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">filter</span></span>
<span id="cb17-36"><a href="data.html#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="data.html#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="data.html#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_pairs(tokenizers):</span>
<span id="cb17-39"><a href="data.html#cb17-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokenize(x, y):</span>
<span id="cb17-40"><a href="data.html#cb17-40" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tokenizers.inputs.tokenize([x])[<span class="dv">0</span>]</span>
<span id="cb17-41"><a href="data.html#cb17-41" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> tokenizers.targets.tokenize([y])[<span class="dv">0</span>]</span>
<span id="cb17-42"><a href="data.html#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="data.html#cb17-43" aria-hidden="true" tabindex="-1"></a>        decoder_inputs <span class="op">=</span> targets[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-44"><a href="data.html#cb17-44" aria-hidden="true" tabindex="-1"></a>        decoder_targets <span class="op">=</span> targets[<span class="dv">1</span>:]</span>
<span id="cb17-45"><a href="data.html#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">dict</span>(encoder_input<span class="op">=</span>inputs, decoder_input<span class="op">=</span>decoder_inputs), decoder_targets</span>
<span id="cb17-46"><a href="data.html#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="data.html#cb17-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenize</span></code></pre></div>
<p>We extract the first batch from the data pipeline:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="data.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb18-2"><a href="data.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer.dataset <span class="im">import</span> Dataset</span>
<span id="cb18-3"><a href="data.html#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="data.html#cb18-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb18-5"><a href="data.html#cb18-5" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb18-6"><a href="data.html#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="data.html#cb18-7" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> Dataset(tokenizers, batch_size<span class="op">=</span>BATCH_SIZE, </span>
<span id="cb18-8"><a href="data.html#cb18-8" aria-hidden="true" tabindex="-1"></a>                  input_seqlen<span class="op">=</span>MAX_LEN, target_seqlen<span class="op">=</span>MAX_LEN)</span>
<span id="cb18-9"><a href="data.html#cb18-9" aria-hidden="true" tabindex="-1"></a>data_train <span class="op">=</span> dataset.data_pipeline(train_examples, </span>
<span id="cb18-10"><a href="data.html#cb18-10" aria-hidden="true" tabindex="-1"></a>                                   num_parallel_calls<span class="op">=</span>tf.data.experimental.AUTOTUNE)    </span>
<span id="cb18-11"><a href="data.html#cb18-11" aria-hidden="true" tabindex="-1"></a>data_eval <span class="op">=</span> dataset.data_pipeline(eval_examples, </span>
<span id="cb18-12"><a href="data.html#cb18-12" aria-hidden="true" tabindex="-1"></a>                                  num_parallel_calls<span class="op">=</span>tf.data.experimental.AUTOTUNE)                           </span>
<span id="cb18-13"><a href="data.html#cb18-13" aria-hidden="true" tabindex="-1"></a>batch1 <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(data_train))</span>
<span id="cb18-14"><a href="data.html#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch1)</span></code></pre></div>
<pre><code>## ({&#39;encoder_input&#39;: &lt;tf.Tensor: shape=(64, 39), dtype=int64, numpy=
## array([[   2,  101,  129, ...,    0,    0,    0],
##        [   2,  108,   44, ...,    0,    0,    0],
##        [   2,   39,   39, ...,    0,    0,    0],
##        ...,
##        [   2,  473, 4392, ...,    0,    0,    0],
##        [   2,   85, 2443, ...,    0,    0,    0],
##        [   2,  932,   91, ...,    0,    0,    0]])&gt;, &#39;decoder_input&#39;: &lt;tf.Tensor: shape=(64, 36), dtype=int64, numpy=
## array([[   2,   45,  125, ...,    0,    0,    0],
##        [   2,   81,   80, ...,    0,    0,    0],
##        [   2,   36,   36, ...,    0,    0,    0],
##        ...,
##        [   2,  116,  267, ...,    0,    0,    0],
##        [   2,  111, 1882, ...,    0,    0,    0],
##        [   2,   78,  596, ...,    0,    0,    0]])&gt;}, &lt;tf.Tensor: shape=(64, 36), dtype=int64, numpy=
## array([[  45,  125, 1082, ...,    0,    0,    0],
##        [  81,   80,  123, ...,    0,    0,    0],
##        [  36,   36,   78, ...,    0,    0,    0],
##        ...,
##        [ 116,  267,  240, ...,    0,    0,    0],
##        [ 111, 1882,   30, ...,    0,    0,    0],
##        [  78,  596, 2960, ...,    0,    0,    0]])&gt;)</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="implementation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="architecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
